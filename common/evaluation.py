import os
import sys
import math

import numpy as np
from PIL import Image
import scipy.linalg

import chainer
import chainer.cuda
from chainer import Variable
from chainer import serializers
import chainer.functions as F
import random

import pdb
from scipy.stats import norm
import tensorflow as tf

sys.path.append(os.path.dirname(__file__))
from AIS.ais import Model, get_schedule
from AIS.priors import NormalPrior
from AIS.kernels import ParsenDensityEstimator
from inception.inception_score import inception_score, Inception

def sample_generate_light(gen, dst, rows=5, cols=5, seed=0):
    @chainer.training.make_extension()
    def make_image(trainer):
        np.random.seed(seed)
        n_images = rows * cols
        xp = gen.xp
        z = Variable(xp.asarray(gen.make_hidden(n_images)))
        with chainer.using_config('train', False), chainer.using_config('enable_backprop', False):
            x = gen(z)
        x = chainer.cuda.to_cpu(x.data)
        np.random.seed()

        x = np.asarray(np.clip(x * 127.5 + 127.5, 0.0, 255.0), dtype=np.uint8)
        _, _, H, W = x.shape
        x = x.reshape((rows, cols, 3, H, W))
        x = x.transpose(0, 3, 1, 4, 2)
        x = x.reshape((rows * H, cols * W, 3))

        preview_dir = '{}/preview'.format(dst)
        preview_path = preview_dir + '/image_latest.png'
        if not os.path.exists(preview_dir):
            os.makedirs(preview_dir)
        Image.fromarray(x).save(preview_path)

    return make_image


def sample_generate(gen, dst, rows=10, cols=10, seed=0):
    """Visualization of rows*cols images randomly generated by the generator."""
    @chainer.training.make_extension()
    def make_image(trainer):
        np.random.seed(seed)
        n_images = rows * cols
        xp = gen.xp
        z = Variable(xp.asarray(gen.make_hidden(n_images)))
        with chainer.using_config('train', False), chainer.using_config('enable_backprop', False):
            x = gen(z)
        x = chainer.cuda.to_cpu(x.data)
        np.random.seed()

        x = np.asarray(np.clip(x * 127.5 + 127.5, 0.0, 255.0), dtype=np.uint8)
        _, _, h, w = x.shape
        x = x.reshape((rows, cols, 3, h, w))
        x = x.transpose(0, 3, 1, 4, 2)
        x = x.reshape((rows * h, cols * w, 3))

        preview_dir = '{}/preview'.format(dst)
        preview_path = preview_dir + '/image{:0>8}.png'.format(trainer.updater.iteration)
        if not os.path.exists(preview_dir):
            os.makedirs(preview_dir)
        Image.fromarray(x).save(preview_path)

    return make_image


def load_inception_model():
    infile = "%s/inception/inception_score.model"%os.path.dirname(__file__)
    model = Inception()
    serializers.load_hdf5(infile, model)
    model.to_gpu()
    return model


def calc_inception(gen, batchsize=100):
    @chainer.training.make_extension()
    def evaluation(trainer):
        model = load_inception_model()

        ims = []
        xp = gen.xp

        n_ims = 50000
        for i in range(0, n_ims, batchsize):
            z = Variable(xp.asarray(gen.make_hidden(batchsize)))
            with chainer.using_config('train', False), chainer.using_config('enable_backprop', False):
                x = gen(z)
            x = chainer.cuda.to_cpu(x.data)
            x = np.asarray(np.clip(x * 127.5 + 127.5, 0.0, 255.0), dtype=np.uint8)
            ims.append(x)
        ims = np.asarray(ims)
        _, _, _, h, w = ims.shape
        ims = ims.reshape((n_ims, 3, h, w)).astype("f")

        mean, std = inception_score(model, ims)

        chainer.reporter.report({
            'inception_mean': mean,
            'inception_std': std
        })

    return evaluation


def get_mean_cov(model, ims, batch_size=100):
    n, c, w, h = ims.shape
    n_batches = int(math.ceil(float(n) / float(batch_size)))

    xp = model.xp

    print('Batch size:', batch_size)
    print('Total number of images:', n)
    print('Total number of batches:', n_batches)

    ys = xp.empty((n, 2048), dtype=xp.float32)

    for i in range(n_batches):
        print('Running batch', i + 1, '/', n_batches, '...')
        batch_start = (i * batch_size)
        batch_end = min((i + 1) * batch_size, n)

        ims_batch = ims[batch_start:batch_end]
        ims_batch = xp.asarray(ims_batch)  # To GPU if using CuPy
        ims_batch = Variable(ims_batch)

        # Resize image to the shape expected by the inception module
        if (w, h) != (299, 299):
            ims_batch = F.resize_images(ims_batch, (299, 299))  # bilinear

        # Feed images to the inception module to get the features
        with chainer.using_config('train', False), chainer.using_config('enable_backprop', False):
            y = model(ims_batch, get_feature=True)
        ys[batch_start:batch_end] = y.data

    mean = chainer.cuda.to_cpu(xp.mean(ys, axis=0))
    # cov = F.cross_covariance(ys, ys, reduce="no").data.get()
    cov = np.cov(chainer.cuda.to_cpu(ys).T)

    return mean, cov

def FID(m0,c0,m1,c1):
    ret = 0
    ret += np.sum((m0-m1)**2)
    ret += np.trace(c0 + c1 - 2.0*scipy.linalg.sqrtm(np.dot(c0, c1)))
    return np.real(ret)

def calc_FID(gen, batchsize=100, stat_file="%s/cifar-10-fid.npz"%os.path.dirname(__file__)):
    """Frechet Inception Distance proposed by https://arxiv.org/abs/1706.08500"""
    @chainer.training.make_extension()
    def evaluation(trainer):
        model = load_inception_model()
        stat = np.load(stat_file)

        n_ims = 10000
        xp = gen.xp
        xs = []
        for i in range(0, n_ims, batchsize):
            z = Variable(xp.asarray(gen.make_hidden(batchsize)))
            with chainer.using_config('train', False), chainer.using_config('enable_backprop', False):
                x = gen(z)
            x = chainer.cuda.to_cpu(x.data)
            x = np.asarray(np.clip(x * 127.5 + 127.5, 0.0, 255.0), dtype="f")
            xs.append(x)
        xs = np.asarray(xs)
        _, _, _, h, w = xs.shape

        with chainer.using_config('train', False), chainer.using_config('enable_backprop', False):
            mean, cov = get_mean_cov(model, np.asarray(xs).reshape((-1, 3, h, w)))
        fid = FID(stat["mean"], stat["cov"], mean, cov)

        chainer.reporter.report({
            'FID': fid,
        })

    return evaluation

class Generator(object):
    def __init__(self, input_dim, output_dim, gen, batchsize, num_sample):
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.gen = gen
        self.batchsize = batchsize
        self.num_sample = num_sample
        
    def __call__(self, z):
        
        xp = self.gen.xp

        for p in range(self.num_sample):
            z_tmp = np.expand_dims(np.expand_dims(z[(p*self.batchsize):((p+1)*self.batchsize)], 2),3)
            z_variable = Variable(xp.asarray(z_tmp.astype('float32')))
            with chainer.using_config('train', False), chainer.using_config('enable_backprop', False):
                x = self.gen(z_variable)

            x = chainer.cuda.to_cpu(x.data)
            x = np.asarray(x, dtype="float32")
            # x = np.asarray(np.clip(x * 127.5 + 127.5, 0.0, 255.0), dtype="float32")
            
            tmp = tf.reshape(tf.constant(x), [self.batchsize, 32*32*3])
            if p == 0:
                output = tmp
            else:
                output = tf.concat([output,tmp], axis = 0)
        return output

class Generator_sample(object):
    def __init__(self, input_dim, output_dim, gen, batchsize, num_sample, dataset=None):
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.gen = gen
        self.batchsize = batchsize
        self.num_sample = num_sample
        self.dataset = dataset
        
    def __call__(self, z):
        output = tf.reshape(tf.constant(self.dataset), [self.batchsize*self.num_sample, 32*32*3])
        # tmp = np.random.rand(self.batchsize*self.num_sample, 32*32*3) * 2. - 1.
        # output = tf.constant(tmp.astype('float32'))
        return output

def calc_AIS(gen, dataset, batchsize=100, latent_shape=128):
    @chainer.training.make_extension()
    def evaluation(trainer):
        num_sample = 10

        generator = Generator(latent_shape, 32*32*3, gen, batchsize, num_sample)
        
        # real_data = dataset.get_example(random.sample(range(10000), batchsize*num_sample))
        # generator = Generator_sample(latent_shape, 32*32*3, gen, batchsize, num_sample, real_data)
        prior = NormalPrior()
        kernel = ParsenDensityEstimator()
        model = Model(generator, prior, kernel, 0.25, num_sample)

        schedule = get_schedule(100, rad=4)
        random_data = np.reshape(dataset.get_example(range(batchsize)), [batchsize, 32*32*3])
        lld = model.ais(random_data, schedule)

        chainer.reporter.report({
            'ais': np.mean(lld)
        })

    return evaluation